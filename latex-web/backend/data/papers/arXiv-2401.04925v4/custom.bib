
@misc{kojima2023large,
      title={Large Language Models are Zero-Shot Reasoners}, 
      author={Takeshi Kojima and Shixiang Shane Gu and Machel Reid and Yutaka Matsuo and Yusuke Iwasawa},
      year={2023},
      eprint={2205.11916},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@misc{lyu2023faithful,
      title={Faithful Chain-of-Thought Reasoning}, 
      author={Qing Lyu and Shreya Havaldar and Adam Stein and Li Zhang and Delip Rao and Eric Wong and Marianna Apidianaki and Chris Callison-Burch},
      year={2023},
      eprint={2301.13379},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{shao2023synthetic,
      title={Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models}, 
      author={Zhihong Shao and Yeyun Gong and Yelong Shen and Minlie Huang and Nan Duan and Weizhu Chen},
      year={2023},
      eprint={2302.00618},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wang2023selfconsistency,
      title={Self-Consistency Improves Chain of Thought Reasoning in Language Models}, 
      author={Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc Le and Ed Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou},
      year={2023},
      eprint={2203.11171},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{zhang2022automatic,
  title={Automatic chain of thought prompting in large language models},
  author={Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
  journal={arXiv preprint arXiv:2210.03493},
  year={2022}
}
@inproceedings{roy2015solving,
  title={Solving General Arithmetic Word Problems},
  author={Roy, Subhro and Roth, Dan},
  booktitle={Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  year={2015},
  organization={Association for Computational Linguistics}
}
@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}
@inproceedings{ling2017program,
  title={Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems},
  author={Ling, Wang and Yogatama, Dani and Dyer, Chris and Blunsom, Phil},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={158--167},
  year={2017}
}
@article{koncel2015parsing,
  title={Parsing algebraic word problems into equations},
  author={Koncel-Kedziorski, Rik and Hajishirzi, Hannaneh and Sabharwal, Ashish and Etzioni, Oren and Ang, Siena Dumas},
  journal={Transactions of the Association for Computational Linguistics},
  volume={3},
  pages={585--597},
  year={2015},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}
@inproceedings{patel2021nlp,
  title={Are NLP Models really able to Solve Simple Math Word Problems?},
  author={Patel, Arkil and Bhattamishra, Satwik and Goyal, Navin},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={2080--2094},
  year={2021}
}
@article{geva2021did,
  title={Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies},
  author={Geva, Mor and Khashabi, Daniel and Segal, Elad and Khot, Tushar and Roth, Dan and Berant, Jonathan},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={346--361},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}


@misc{fu2023complexitybased,
      title={Complexity-Based Prompting for Multi-Step Reasoning}, 
      author={Yao Fu and Hao Peng and Ashish Sabharwal and Peter Clark and Tushar Khot},
      year={2023},
      eprint={2210.00720},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@article{openai2023gpt,
  title={Gpt-4 technical report. arxiv 2303.08774},
  author={OpenAI, R},
  journal={View in Article},
  volume={2},
  pages={13},
  year={2023}
}

@article{madaan2022text,
  title={Text and patterns: For effective chain of thought, it takes two to tango},
  author={Madaan, Aman and Yazdanbakhsh, Amir},
  journal={arXiv preprint arXiv:2209.07686},
  year={2022}
}
@article{tang2023large,
  title={Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners},
  author={Tang, Xiaojuan and Zheng, Zilong and Li, Jiaqi and Meng, Fanxu and Zhu, Song-Chun and Liang, Yitao and Zhang, Muhan},
  journal={arXiv preprint arXiv:2305.14825},
  year={2023}
}

@article{li2023dissecting,
  title={Dissecting Chain-of-Thought: A Study on Compositional In-Context Learning of MLPs},
  author={Li, Yingcong and Sreenivasan, Kartik and Giannou, Angeliki and Papailiopoulos, Dimitris and Oymak, Samet},
  journal={arXiv preprint arXiv:2305.18869},
  year={2023}
}
@misc{merrill2023expressive,
      title={The Expressive Power of Transformers with Chain of Thought}, 
      author={William Merrill and Ashish Sabharwal},
      year={2023},
      eprint={2310.07923},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{wu2023analyzing,
  title={Analyzing chain-of-thought prompting in Large language models via gradient-based feature Attributions},
  author={Wu, Skyler and Shen, Eric Meng and Badrinath, Charumathi and Ma, Jiaqi and Lakkaraju, Himabindu},
  journal={arXiv preprint arXiv:2307.13339},
  year={2023}
}


@misc{feng2023revealing,
      title={Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective}, 
      author={Guhao Feng and Bohang Zhang and Yuntian Gu and Haotian Ye and Di He and Liwei Wang},
      year={2023},
      eprint={2305.15408},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{schaeffer2023emergent,
  title={Are emergent abilities of Large Language Models a mirage?},
  author={Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  journal={arXiv preprint arXiv:2304.15004},
  year={2023}
}

@misc{yao2023tree,
      title={Tree of Thoughts: Deliberate Problem Solving with Large Language Models}, 
      author={Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik Narasimhan},
      year={2023},
      eprint={2305.10601},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{ge2023openagi,
title={Open{AGI}: When {LLM} {M}eets {D}omain {E}xperts},
author={Yingqiang Ge and Wenyue Hua and Kai Mei and jianchao ji and Juntao Tan and Shuyuan Xu and Zelong Li and Yongfeng Zhang},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023}
}

@misc{jin2024exploring,
      title={Exploring Concept Depth: How Large Language Models Acquire Knowledge at Different Layers?}, 
      author={Mingyu Jin and Qinkai Yu and Jingyuan Huang and Qingcheng Zeng and Zhenting Wang and Wenyue Hua and Haiyan Zhao and Kai Mei and Yanda Meng and Kaize Ding and Fan Yang and Mengnan Du and Yongfeng Zhang},
      year={2024},
      eprint={2404.07066},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}