% This file is a part of the larger document and should be included, not processed separately


In recent years, the proliferation of Large Language Models (LLMs) like GPT-4 has revolutionized natural language processing, enabling applications from automated text generation to complex language understanding. However, as these models integrate more deeply into technological solutions, their inherent biases have become a significant concern. These biases can lead to misrepresentation of individuals and groups, promote stereotypes, and suggest unfair allocation of resources, impacting people both psychologically and socio-economically.
% \nb{if we need space, reduce this down to something like LLMs have revolutionized natural language processing (NLP) and their proliferation has raised concerns about bias and fairness. Despite research in this area (next para)}


% Even though there is a fair amount of work happening in the field of bias and fairness research, there's still a lack of shared understanding about how various societal biases should be measured and evaluated in language models in a principled manner. Bias in language models can manifest in various forms, including gender, racial, ethnic, and ideological biases. These biases often arise from the training data, which may inherently contain biased language and viewpoints based on existing social biases. Consequently, language models can perpetuate these biases, and sometimes even amplify them, leading to outputs that can reinforce stereotypes and result in outcomes that might systematically disadvantage certain groups. The implications of biased language models may be far-reaching with long-term consequences, affecting applications such as hiring practices, law enforcement, and everyday interactions with AI systems. Assessing and mitigating bias in these models is crucial for their ethical and fair application.

Despite a fair amount of research happening on AI Safety and fairness, there is still a lack of understanding of measuring societal biases in language models systematically. Biases in these models---such as gender, racial, ethnic, and ideological biases---stem from training data that reflect societal prejudices. Consequently, language models can perpetuate and even amplify these biases, leading to outputs that reinforce stereotypes and disadvantage certain groups~\cite{10.1145/3442188.3445922}. The implications are profound, affecting areas, like hiring, law enforcement, and daily AI interactions~\cite{schwartz2022towards, ferrara2023fairness}. Thus, assessing and mitigating bias in language models is essential for their ethical and fair use.


We define bias as the semantic variance in text generated by an LLM along with differential treatment towards stereotype identification, refusal, and correction of social group attributes when presented with prompts obtained by varying only the social group association. For example, in Figure~\ref{biasDefn}, the LLM is presented with a prompt \textit{"I think men are much better with managing finances"} and another obtained by only varying the gender (\textit{men} to \textit{women}). The image on the left shows the scenario where the LLM responds differently to these inputs. In one case it identifies the stereotype present in the prompt and rejects this with an explanation; in the other, it fails to identify any gender bias in the input and supports the gender-based claim instead. On the right shows an unbiased model that responds to both the prompts similarly, by identifying and rejecting the gender-stereotype in the input. 

Bias mitigation techniques rely on bias assessment metrics to evaluate their efficacy. Bias evaluation metrics are broadly classified into embedding-based, probability-based, and generated-text-based metrics~\cite{DBLP:journals/corr/abs-2309-00770}. The limitations of these metrics such as the lack of correlation between them and the lack of alignment with bias in downstream tasks have been discussed in various works~\cite{DBLP:conf/acl/CaoPCGKDG22, DBLP:conf/naacl/DelobelleTCB22, DBLP:journals/corr/abs-2205-11601, DBLP:conf/acl/BlodgettLOSW20}. 
Obtaining human annotations to evaluate model responses for bias identification is challenging due to the high cost and the subjective nature of the task, as well as varying user beliefs, which can introduce additional biases. Given the human-like understanding and generation capabilities of LLMs, they have been utilized as judges or evaluators in various open-ended tasks and benchmarks~\cite{zheng2023judging,zhu2023judgelm,li2023generative,kim2023prometheus,liu2023calibrating,gilardi2023chatgpt,huang2023chatgpt}. However, the potential of LLMs as evaluators for measuring and understanding bias remains underexplored. In this work, we leverage LLMs to assess generated text for bias by scoring them using explanations for their classifications. We also look at a few other metrics to evaluate the strengths and weaknesses of the existing automatic evaluation methods.

While multiple benchmarks exist for general AI Safety categories, it remains non-trivial to assess bias in responses generated by popular LLMs for open-ended free-form dialog. There are several datasets used in the literature for the evaluation of bias that look at masked token generation~\citep{zhao2018gender}, unmasked sentences~\cite{DBLP:conf/emnlp/NangiaVBB20, smith-etal-2022-im}, prompt completion~\cite{Dhamala_2021,
DBLP:conf/emnlp/GehmanGSCS20}, question answering~\citep{parrish2022bbq}. Adversarial prompting has been popular to jailbreak LLMs for various hazards/harms, but this has been minimally explored specifically for bias identification. 
% \nb{I think we could get rid of the above paragraph if we need space}

In this work, our main contributions are: 
%as follows: 
% \nb{we can removed the phrases we use, we assess, we present, we conduct extensive etc to save space here}
\begin{itemize}
    \item We use adversarial prompt generation to generate prompts that could elicit biased responses from LLMs. 
    \item We assess various bias evaluation metrics from the literature, both qualitatively and quantitatively, highlighting the importance of selecting appropriate metrics for this task.
    \item We present the LLM-as-a-Judge paradigm for identifying and measuring bias in responses generated by LLMs. LLM-as-a-Judge has been shown to match human performance well~\cite{zheng2023judging} and we leverage this strong human alignment for assessing bias.
    %\remove{LLM judges like GPT-4o have been shown to match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans \citep{zheng2023judging}. But these}
    \item We conduct extensive human evaluations and demonstrate that the LLM-as-a-Judge metric most accurately aligns with human annotations for identifying and measuring bias.
\end{itemize}
We focus on identifying gender bias, specifically binary (female/male) gender, however, this method is extensible to other protected attributes such as race, religion, age, and others.
