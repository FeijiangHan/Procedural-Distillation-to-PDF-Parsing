% This file is a part of the larger document and should be included, not processed separately


Table~\ref{results} shows the performance of the different target LLMs when prompted with the individual adversarial Male/Female (M/F) prompts generated by the attacker LLMs.

With Llama3 as the attacker model, we observe that the Perspective API scores for Identity Attack, Insult, and Toxicity scores for female responses are significantly higher when compared to the male responses across all models. Further, Mixtral, Mistral, and GPT-4 models show lower identity attack and toxicity scores, on average, compared to the Llama2 model family. These results suggest that female oriented responses may require more alignment and correction for these metrics.  

We see that for Mixtral 8x7B Inst, which is a powerful Mixture of Experts model, the Female response sentiment is significantly lower than the Male response sentiment, with Mistral and GPT-4 following a similar trend. This suggests that these models generate more critical responses with negative polarity for female gender. We also observe the highest mean bias scores using LLM-as-a-Judge (M/F =0.88/0.88) in the Mistral 7B Inst model implying that this model, on average, generates more biased responses. Both findings correlate with the DecodingTrust platform Fairness metric on the leader-board (showing the lowest numbers for GPT-4 and Mistral models). 
%See Table \ref{tab:mistral_bias} in Appendix~\ref{appendix-biased} for an example response from the model that fully complies with the prompt and generates biased responses without acknowledging biases. 

For the Llama family of target LLMs, we see that (M/F) sentiment difference, as well as (M/F) bias score difference from LLM-as-a-Judge, reduces with an increase in model size, which reinforces the idea that larger Llama2 models are fairer than their smaller versions.

Regard scores in the table are computed by evaluating male responses with reference to female responses. We see that male responses have a negative regard w.r.t female responses in Llama2-70B and GPT-4, which aligns with the increase in bias scores for male responses from LLM-judge.
% with Female response sentiment going higher than Male in the 13b and 70b variants. 
We also assessed LlamaGuard2's ability to detect gender bias (categorized under `hate'). However, we found it ineffective, as it failed to classify biased language as unsafe, hence we do not report the results here.


We provide more details on human evaluation\footnote{We chose crowd workers, who had >95\% HIT approval rate and from US and Canada locations.} for Task 1 in Appendix~\ref{appendix-humaneval}. We compute the inter-annotator agreement for the annotations using Cohen's kappa score and find that the agreement on the different questions is quite low, falling in the 0.01 - 0.20 (slight agreement) range. This underscores the complexity and subjectivity of the task. 


We present several examples in Appendix~\ref{appendix-eval} with LLM Judgement scores and highlight a few examples using OpenAI Compliance Annotation in Appendix~\ref{appendix-compliance}. 
% These examples showcase where the models comply with a gendered prompt while refusing to comply with its counterfactual; thereby showing biases in response. 
  

\subsection{Overall Bias Analysis}
Following our definition of bias in Figure~\ref{biasDefn}, our second AMT task involved presenting both male and female prompt-response pairs to the users who would assess whether the responses in each pair conveyed similar or substantially different ideas.
We hypothesize that a higher proportion of gendered response pairs marked as dissimilar indicates greater bias in the model. After aggregating responses through majority voting, we calculated the percentage of gendered prompt-response pairs that crowd-workers classified as "conveying different ideas."
Table~\ref{tab:overallbias} presents the results of this comprehensive bias evaluation, quantifying the degree to which the model's responses differ based on gender-specific prompts, and we compare these to the Sentiment-Gap and LLM-judge Gap scores. 

We observe that bias, based on responses conveying different ideas from human evaluation, is highest for the Llama2-7b-chat model, which is also reflected by the LLM-judge Gap score. All metrics consistently show that overall bias decreases as the model size increases within the Llama model family. Notably, there is a 100\% agreement in the trend of diminishing bias between the human bias score and the LLM-judge Gap score: Llama2-7b-chat (highest), Llama2-13b, Mistral, Mixtral, Llama2-70B, and GPT-4 (lowest). However, overall bias based on the Sentiment Gap score is highest for Mixtral and lowest for Llama2-70b. This observation indicates that the LLM-judge Gap score aligns with human judgment of bias in model generation.


% We see higher bias scores for male responses in Llama2-13B, 70B, and GPT-4, and higher bias scores for female responses in Llama2-7b and Mixtral. 
% \nb{comment: This is out of place as it refers to table 1 again.}
 
%Meanwhile, male responses show positive regard in Llama2-13B and Mixtral models. 


% Figure \ref{fig:identityAttack} shows that these differences between male and female responses in identity attack scores are statistically significant. 

% 7b- male netural with female as baseline, female positive regard with male as baseline. 
% 13b-Male positive regard with female as baseline
% 70b- male negative with female as baseline
% mixtral - male positive regard with female as baseline
% mistral - 
% GPT-4- male negative regard with female baseline



% However, we found that the subtlety of the biased language prevented LlamaGuard from classifying any responses as unsafe. Consequently, we determined that this is not an effective metric for bias detection. 


% Overall, we observe that different metrics lead to varying nuances in understanding gender biases, with scores often not well aligned and each metric having its own shortcomings. We present this observation with several examples in Appendix \ref{appendix-eval} with LLM Judgment scores (along with other evaluation scores). This highlights the challenges in bias measurement and the need for human evaluation and a reliable bias identification metric

