% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@phdthesis{butterly2017gemini,
  title={Gemini: Technical Report},
  author={Butterly, Adam},
  year={2017},
  school={Dublin, National College of Ireland}
}

@article{aryabumi2024aya,
  title={Aya 23: Open weight releases to further multilingual progress},
  author={Aryabumi, Viraat and Dang, John and Talupuru, Dwarak and Dash, Saurabh and Cairuz, David and Lin, Hangyu and Venkitesh, Bharat and Smith, Madeline and Marchisio, Kelly and Ruder, Sebastian and others},
  journal={arXiv preprint arXiv:2405.15032},
  year={2024}
}

@article{abdin2024phi,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}

@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@article{chao2023jailbreaking,
  title={Jailbreaking black box large language models in twenty queries},
  author={Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J and Wong, Eric},
  journal={arXiv preprint arXiv:2310.08419},
  year={2023}
}

@misc{Anthropic,
title={The Claude 3 Model Family: Opus, Sonnet, Haiku},
author={Anthropic},
year={2024},
month={March},
url={https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf},
publisher={Anthropic}
}

@article{ding2023wolf,
  title={A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily},
  author={Ding, Peng and Kuang, Jun and Ma, Dan and Cao, Xuezhi and Xian, Yunsen and Chen, Jiajun and Huang, Shujian},
  journal={arXiv preprint arXiv:2311.08268},
  year={2023}
}

@article{inan2023llama,
  title={Llama guard: Llm-based input-output safeguard for human-ai conversations},
  author={Inan, Hakan and Upasani, Kartikeya and Chi, Jianfeng and Rungta, Rashi and Iyer, Krithika and Mao, Yuning and Tontchev, Michael and Hu, Qing and Fuller, Brian and Testuggine, Davide and others},
  journal={arXiv preprint arXiv:2312.06674},
  year={2023}
}

@article{foo2024lionguard,
  title={LionGuard: Building a Contextualized Moderation Classifier to Tackle Localized Unsafe Content},
  author={Foo, Jessica and Khoo, Shaun},
  journal={arXiv preprint arXiv:2407.10995},
  year={2024}
}

@misc{aya,
      title={The Multilingual Alignment Prism: Aligning Global and Local Preferences to Reduce Harm}, 
      author={Aakanksha and Arash Ahmadian and Beyza Ermis and Seraphina Goldfarb-Tarrant and Julia Kreutzer and Marzieh Fadaee and Sara Hooker},
      year={2024},
      eprint={2406.18682},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.18682}, 
}

@article{mazeika2024harmbench,
  title={Harmbench: A standardized evaluation framework for automated red teaming and robust refusal},
  author={Mazeika, Mantas and Phan, Long and Yin, Xuwang and Zou, Andy and Wang, Zifan and Mu, Norman and Sakhaee, Elham and Li, Nathaniel and Basart, Steven and Li, Bo and others},
  journal={arXiv preprint arXiv:2402.04249},
  year={2024}
}

@misc{zou2023universal,
      title={Universal and Transferable Adversarial Attacks on Aligned Language Models}, 
      author={Andy Zou and Zifan Wang and J. Zico Kolter and Matt Fredrikson},
      year={2023},
      eprint={2307.15043},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{vidgen2024introducing,
  title={Introducing v0. 5 of the ai safety benchmark from mlcommons},
  author={Vidgen, Bertie and Agrawal, Adarsh and Ahmed, Ahmed M and Akinwande, Victor and Al-Nuaimi, Namir and Alfaraj, Najla and Alhajjar, Elie and Aroyo, Lora and Bavalatti, Trupti and Blili-Hamelin, Borhane and others},
  journal={arXiv preprint arXiv:2404.12241},
  year={2024}
}


@article{bhardwaj2023red,
  title={Red-teaming large language models using chain of utterances for safety-alignment},
  author={Bhardwaj, Rishabh and Poria, Soujanya},
  journal={arXiv preprint arXiv:2308.09662},
  year={2023}
}

@article{qi2023fine,
  title={Fine-tuning aligned language models compromises safety, even when users do not intend to!},
  author={Qi, Xiangyu and Zeng, Yi and Xie, Tinghao and Chen, Pin-Yu and Jia, Ruoxi and Mittal, Prateek and Henderson, Peter},
  journal={arXiv preprint arXiv:2310.03693},
  year={2023}
}

@article{bhardwaj2024language,
  title={Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic},
  author={Bhardwaj, Rishabh and Anh, Do Duc and Poria, Soujanya},
  journal={arXiv preprint arXiv:2402.11746},
  year={2024}
}

@article{andriushchenko2024does,
  title={Does Refusal Training in LLMs Generalize to the Past Tense?},
  author={Andriushchenko, Maksym and Flammarion, Nicolas},
  journal={arXiv preprint arXiv:2407.11969},
  year={2024}
}

@article{bhardwaj2023language,
  title={Language model unalignment: Parametric red-teaming to expose hidden harms and biases},
  author={Bhardwaj, Rishabh and Poria, Soujanya},
  journal={arXiv preprint arXiv:2310.14303},
  year={2023}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{zheng2024judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{wolf2019huggingface,
  title={Huggingface's transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}

@article{ahmadian2024multilingual,
  title={The Multilingual Alignment Prism: Aligning Global and Local Preferences to Reduce Harm},
  author={Ahmadian, Arash and Ermis, Beyza and Goldfarb-Tarrant, Seraphina and Kreutzer, Julia and Fadaee, Marzieh and Hooker, Sara and others},
  journal={arXiv preprint arXiv:2406.18682},
  year={2024}
}

@article{rottger2023xstest,
  title={Xstest: A test suite for identifying exaggerated safety behaviours in large language models},
  author={R{\"o}ttger, Paul and Kirk, Hannah Rose and Vidgen, Bertie and Attanasio, Giuseppe and Bianchi, Federico and Hovy, Dirk},
  journal={arXiv preprint arXiv:2308.01263},
  year={2023}
}

@article{dubey2024llama,
  title={The Llama 3 Herd of Models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{team2024gemma,
  title={Gemma 2: Improving Open Language Models at a Practical Size},
  author={Team, Gemma and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and others},
  journal={arXiv preprint arXiv:2408.00118},
  year={2024}
}

@article{ran2024jailbreakeval,
  title={JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts Against Large Language Models},
  author={Ran, Delong and Liu, Jinyuan and Gong, Yichen and Zheng, Jingyi and He, Xinlei and Cong, Tianshuo and Wang, Anyu},
  journal={arXiv preprint arXiv:2406.09321},
  year={2024}
}

@article{zhou2024easyjailbreak,
  title={EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models},
  author={Zhou, Weikang and Wang, Xiao and Xiong, Limao and Xia, Han and Gu, Yingshuang and Chai, Mingxu and Zhu, Fukang and Huang, Caishuang and Dou, Shihan and Xi, Zhiheng and others},
  journal={arXiv preprint arXiv:2403.12171},
  year={2024}
}

@inproceedings{lhoest-etal-2021-datasets,
    title = "Datasets: A Community Library for Natural Language Processing",
    author = "Lhoest, Quentin  and
      Villanova del Moral, Albert  and
      Jernite, Yacine  and
      Thakur, Abhishek  and
      von Platen, Patrick  and
      Patil, Suraj  and
      Chaumond, Julien  and
      Drame, Mariama  and
      Plu, Julien  and
      Tunstall, Lewis  and
      Davison, Joe  and
      {\v{S}}a{\v{s}}ko, Mario  and
      Chhablani, Gunjan  and
      Malik, Bhavitvya  and
      Brandeis, Simon  and
      Le Scao, Teven  and
      Sanh, Victor  and
      Xu, Canwen  and
      Patry, Nicolas  and
      McMillan-Major, Angelina  and
      Schmid, Philipp  and
      Gugger, Sylvain  and
      Delangue, Cl{\'e}ment  and
      Matussi{\`e}re, Th{\'e}o  and
      Debut, Lysandre  and
      Bekman, Stas  and
      Cistac, Pierric  and
      Goehringer, Thibault  and
      Mustar, Victor  and
      Lagunas, Fran{\c{c}}ois  and
      Rush, Alexander  and
      Wolf, Thomas",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-demo.21",
    pages = "175--184",
    abstract = "The scale, variety, and quantity of publicly-available NLP datasets has grown rapidly as researchers propose new tasks, larger models, and novel benchmarks. Datasets is a community library for contemporary NLP designed to support this ecosystem. Datasets aims to standardize end-user interfaces, versioning, and documentation, while providing a lightweight front-end that behaves similarly for small datasets as for internet-scale corpora. The design of the library incorporates a distributed, community-driven approach to adding datasets and documenting usage. After a year of development, the library now includes more than 650 unique datasets, has more than 250 contributors, and has helped support a variety of novel cross-dataset research projects and shared tasks. The library is available at https://github.com/huggingface/datasets.",
    eprint={2109.02846},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
}